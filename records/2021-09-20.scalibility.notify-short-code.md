# Move the Notify API from Kubernetes to AWS Lambda

Date: 2021-08-16

## Status

**ACCEPTED**

## Context

The SRE team is encouraging us to move the Notify API from Kubernetes to AWS Lambda. AWS Lambda functions allow code to be run in a highly scalable way - we can have hundreds of lambda functions running simulateously, and this auto-scales to meet the number of requests being made to the API. The benefits of doing this are thought to be the following:

- less complexity for the Notify team to manage, compared to Kubernetes
- more scalable API
- less costly compared to kubernetes + Amazon EKS

At the time when Notify was being deployed by CDS, AWS Lambda had a smaller maximum size for a lambda function. The notification-api codebase was over this size limit, so it was deployed a different way.

Since then, AWS has added support for Docker images and increased the size limit for these to 10 GB. This means it is now possible to deploy the Notify API as a Lambda function.

CDS has now used lambda functions to run the [Covid Alert metrics collection and aggregation](https://github.com/cds-snc/covid-alert-metrics-terraform/tree/main/aws). Lambda functions helped that part of the service to scale well.

## Proposed architecture

AWS Lambda (with some amount of provisioned concurrency) + API Gateway

## Additional considerations

AWS Lambda functions exhibit "cold-start" behaviour, where they can take more than 1 second to respond if they [have not been invoked recently](https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/#:~:text=The%20duration%20of%20a%20cold,test%20functions%20than%20production%20workloads). If we do not mitigate this, the admin site will appear unresponsive at times. AWS has a feature to mitigate this, called [provisioned concurrency](https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/). We can also "auto-scale" the amount of provisioned concurrency based on how the current utilization ([ref](https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-api)).

Another consideration will be the effects on other parts of Notify. If the API can scale much better than in the past, will some other parts of the system become overloaded? This may not be an issue because we queue our upcoming jobs with Celery, but we should do significant load testing before running lambda functions in production.

Lambdas may make local testing a bit more difficult by forcing us to either wire up some docker containers using aws-lambda-rie or use AWS SAM to orchestrate the work. With Kubernetes, testing locally is easier since docker desktop can be used to bring up the test system.

## Load testing results

We ran a load test where we simulated 5,500 requests per minute to the api using locust, to see if the new lambda deployment could meet existing peak loads. The these requests were made to the `/v2/notifications/email` endpoint and were split into 2 services to make the test more realistic.

Results are discussed below.

### Existing Kubernetes + EKS deployment

There were 13,840 failures (7668+6172) out of 39,052 requests (21092+17960), which is a failure rate of 35%. Median response time was 15000ms for Service 1 and 12000ms for Service 2. 99th percentile response time was 46000ms for Service 1 and 42000ms for Service 2 over a 7 minute test.

- [Report for Service 1 (k8s)](https://htmlpreview.github.io/?https://raw.githubusercontent.com/cds-snc/notification-adr/move-to-lambda/records/attachments/k8s_service_1_report_1629310371.616257.html)
- [Report for Service 2 (k8s)](https://htmlpreview.github.io/?https://raw.githubusercontent.com/cds-snc/notification-adr/move-to-lambda/records/attachments/k8s_service_2_report_1629310332.8675654.html)

### Lambda + API Gateway deployment

There were 0 failures, and median response time was 110ms for both services and 99th percentile response time was 250ms for Service 1 and 230ms for Service 2 over a 7 minute test.

- [Report for Service 1 (lambda)](https://htmlpreview.github.io/?https://raw.githubusercontent.com/cds-snc/notification-adr/move-to-lambda/records/attachments/lambda_service_1_report_1629229241.7761497.html)
- [Report for Service 2 (lambda)](https://htmlpreview.github.io/?https://raw.githubusercontent.com/cds-snc/notification-adr/move-to-lambda/records/attachments/lambda_service_2_report_1629229258.5504797.html)


We had provisioned 2 concurrent lambda functions to always run, and these handled about 10% of the requests, with the other 90% going to lambda functions that were provisioned on-demand.

### Analysis

The only constraint on this test was that the number of requests per minute remained constant. Within each minute the request rate varied considerably, and sometimes exceeded 100 requests per second. This greatly exceeds the request rates we are currently seeing in production. So it is not unexpected that the Kubernetes deployment performed poorly. However, this shows that the lambda deployment can scale much more compared to our current EKS/machines/pods configuration.

## Decision

Move the API from the Kubernetes/EKS deployment to Lambda + API Gateway. 

