# Upgrade GC Notify Postgres to Latest Major Version

Date: 2023-12-19

## Status

**APPROVED**.

## Related ADRs

[Previous Database Upgrade ADR](https://github.com/cds-snc/notification-adr/blob/main/records/2022-12-13.upgrade-database.md)

## Context

[Staging](https://ca-central-1.console.aws.amazon.com/rds/home?region=ca-central-1#database:id=notification-canada-ca-staging-cluster;is-cluster=true;tab=configuration) and
[production](https://ca-central-1.console.aws.amazon.com/rds/home?region=ca-central-1#database:id=notification-canada-ca-production-cluster;is-cluster=true;tab=configuration) are both running aurora-postgresql 11.21 (from 2023-10-24). The latest version of PostgreSQL available on AWS is 15.5. The latest version of PostgreSQL is 16.2.

AWS announced the end of life for the 11.x version of Aurora Postgres on February 29th 2024. After this date, Notify will be charged an additional fee to receive extended support and no more patches would be provided.

It is highly recommended that GC Notify upgrade to a supported major version.

## Options

### Major version upgrades

Major version upgrades can contain database changes that are not backward-compatible with existing applications. As a result, you must manually perform major version upgrades of your DB instances. You can initiate a major version upgrade by modifying your DB instance. However, before you perform a major version upgrade, we recommend that you follow the steps described in [Choosing a major version upgrade for PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.PostgreSQL.html#USER_UpgradeDBInstance.PostgreSQL.MajorVersion). During a major version upgrade, Amazon RDS also upgrades all of your in-Region read replicas along with the primary DB instance.

### Minor version upgrades

In contrast, minor version upgrades include only changes that are backward-compatible with existing applications. You can initiate a minor version upgrade manually by modifying your DB instance. Or you can enable the Auto minor version upgrade option when creating or modifying a DB instance. Doing so means that your DB instance is automatically upgraded after Amazon RDS tests and approves the new version. If your PostgreSQL DB instance is using read replicas, you must first upgrade all of the read replicas before upgrading the primary instance. If your DB instance is in a Multi-AZ deployment, then the writer and any standby replicas are upgraded simultaneously. Therefore, your DB instance might not be available until the upgrade is complete. For more details, see [Automatic minor version upgrades for PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.PostgreSQL.html#USER_UpgradeDBInstance.PostgreSQL.Minor). For information about manually performing a minor version upgrade, see [Manually upgrading the engine version](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Upgrading.html#USER_UpgradeDBInstance.Upgrading.Manual).

### Upgrade to 12.x

To minimize risk, GC Notify could upgrade to Postgres 12.x in the short term. This would provide an additional year of being within the standard AWS support window, as the end of life target for Aurora Postgres 12.x is February 28th 2025.

### Upgrade to 15.x

GC Notify could also upgrade to the latest Postgres 15.x, which would provide a longer period of support (Standard support ends 29 February 2028 for Postgres 15). This would also allow Notify to take advantage of newer features offered by both Aurora and Postgres such as improved zero downtime minor version upgrades.

### Initial Decision

After reviewing with the team, it was established that there were no changes that would break GC Notify by upgrading to 15.x immediately. All dependencies support Postgres 15 with their current versions. Since the level of effort to upgrade to 12 or 15 was the same, it was decided to upgrade to the latest available version.

## Upgrade Process

### Blue/Green ðŸ”µ/ðŸŸ¢ Deployments

Upgrading to Postgres 15 without causing significant downtime required the use of the blue/green deployment concept offered by AWS. A typical blue/green deployment does the following:

1. AWS creates a Blue/Green Deployment group in RDS.
1. AWS associates the existing database cluster to this deployment group as the "Blue" database.
1. AWS creates a new database cluster under the deployment group using the existing database's data and labels it the "green" database.
1. AWS configures logical replication between the blue and green databases so that all new data to blue is automatically sent to green. **At this point only the blue database is still in active use**.
1. AWS begins the upgrade process on the green database to upgrade to the desired version
1. When complete, the notify team can connect applications to the green database manually to test the upgrade.
1. Once testing is completed, the Notify team can trigger a switchover between blue and green.
1. When switchover is initiated, AWS cuts traffic to the blue database and waits for all replication to complete in the green database. **There is a short amount of downtime of 10-15 seconds associated with this step**.
1. When replication is complete, AWS enables the green database as the active database.
1. AWS renames the blue database, appending "-old" to the name.
1. AWS renames the green database, reusing the previous blue name.
1. The Notify team deletes the blue/green deployment group, which deletes the "old" blue database.

### Problems

There are two problems with the blue/green upgrade specific to Notify:

1. The notify databases does not have logical replication enabled. This is easily fixed by adding the enable flag in Terraform, however it requires a database restart to take effect, thus creating an additional 10 seconds of downtime.

2. The blue/green deployment strategy does not support the usage of an RDS proxy. Notify uses an RDS proxy to improve database performance, since the RDS proxy is better able to manage a large number of concurrent connections to the database from the celery workers and API lambdas. It will be necessary to remove the RDS Proxy temporarily so that the blue/green deployment can be performed.

## Steps

The following upgrade process was established to mitigate the aforementioned problems while performing the database upgrade.

### Pre-Requisites

#### Step 1 - Enable Logical Replication

Notify Core will make a release that sets the enable flag on logical replication, and schedule a restart of the database several days before performing the blue/green deployment upgrade. This results in approximately 10 seconds of downtime and will set off some alarms for support to acknowledge.

### Upgrade The Database

#### Step 2 - Configure Notify To Bypass the RDS Proxy

Notify Core will create a special branch in the Terraform and Kubernetes-Manifest code that will change the database URL from the RDS proxy endpoint to the direct database endpoint.

In Terraform, the following resources are affected:

- RDS is modified to provide the direct database endpoint as an output.
- Lambda-API is modified to read the database URL from the above RDS output.
- Database-Tools is likewise modified to read the database URL from the RDS output.

Once this branch is applied, Notify Core will verify that the new API Lambda function alias is set, and that the database URL is set to the direct database endpoint.

In Kubernetes-Manifests, a new branch is created with the encrypted variables file changed in the following places:

- `POSTGRES_SQL`
- `SQLALCHEMY_DATABASE_READER_URI`

#### Step 3 - Remove The RDS Proxy and Disable Delete Protection

Notify Core will create a new branch in Terraform that deletes the RDS proxy and disables delete protection. Delete protection must be disabled when performing the cleanup of the blue/green deployment.

#### Step 4 - Create The Blue/Green Deployment

A shell command is run outside of Terraform to [create the blue/green deployment](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-creating.html). The reason this is done outside of terraform is that this is an ephemeral step, and Terraform does not support this use case.

#### Step 5 - Perform The Blue/Green Switchover

A shell command is run outside of Terraform to switch from the Blue to the Green database. This results in approximately 15 seconds of downtime.

#### Step 6 - Delete The Blue/Green Deployment

A shell command is run outside of Terraform to remove the Blue/Green deployment. This is where deletion protection must be disabled.

#### Step 7 - Recreate The RDS Proxy and Reconfigure GC Notify

Notify Core will create a new branch in Terraform that recreates the RDS proxy and then points API Lambda, database tools, and the Kubernetes Manifests to point back to this RDS Proxy.

In Terraform, the following resources are affected:

- RDS is modified to add the RDS Proxy back and set the parameter group family to postgres15.
- Lambda-API is modified to use the RDS proxy.
- Database-Tools is likewise modified to use the RDS proxy.

In Kubernetes-Manifests, a new branch is created with the encrypted variables file changed in the following places:

- `POSTGRES_SQL`
- `SQLALCHEMY_DATABASE_READER_URI`

### Terraform

Note that our [terraform](https://github.com/cds-snc/notification-terraform/blob/main/aws/rds/rds.tf#L72) sets the engine version to 11.9, but adds the engine version to the `ignore_changes` section. So we do not have to change our terraform to change the engine version.

If we continue to do manual database updates, we should revisit this decision.

## Testing

We enlarged our dev database to be the same size as our prod system by adding rows to the `notification_history` table (total of 126M rows). We then [ran a soak test](https://gcdigital.slack.com/archives/C068JQZ45UP/p1706204048668619) (one email / sec) during the steps to identify issues that couple arise during the production migration. Results were as follows:

| Step                             | Time   | Downtime | Lost Notifications | Other Issues
| -------------------------------- | ------ | -------- | ------------------ | ------------
|    1. Remove RDS proxy           | 21 min |     none |               none | none
|    2. Create blue/green          | 38 min |     none |               none | one API gateway timeout
|    3. Switch from blue to green  |  2 min |   14 sec |               none | 15 notifications stuck "created"
|    4 remove blue/green           | 12 min |     none |               none | none
|    5 restore RDS proxy           | 25 min |     none |               none | none

### Notes

- On step 3 and 5 there were a few notifications that stalled in "sending" before being retried a few minutes later (5 for step 3, 2 for step 5).
- We believe that the API gateway failure during step 2 was not related to the migration.
- The stuck notifications may represent 15 seconds of notifications rather than a fixed number of 15.
- These stuck notifications will be sent after a 4 hour, 15 minute delay. This may be unacceptable; if so we should decide how to reduce the delay.

## Consequences

_TODO: Describe foreseen and felt consequences of the decision (possible after 1-3 months)._
