# Performance testing

Date: 2021-11-30

## Status

**ACCEPTED**

Change happens regularly. A fundamental uncertainty that change presents before software projects with large concurrency demand (like Notify) is performance regression, this is very likely to happen when change regresses the performance. In order to ensure that performance regression is identified pre-deployment [[1]](https://docs.microsoft.com/en-us/archive/blogs/dajung/ebook-pnp-performance-testing-guidance-for-web-applications), performance indices need to be produced for evaluation to confirm if Notify can handle the expected requests per second or data size.

## Context

Notify needs a performance testing reporting solution to inform the Team and other stakeholders about how well the recent changes have either improved or impacted it.

The fundamental aim is to prove that Notify meets response time SLA, acceptable error rates and performance expectations before deploying changes to production[[1]](https://docs.microsoft.com/en-us/archive/blogs/dajung/ebook-pnp-performance-testing-guidance-for-web-applications). 

### Why?

Performance testing is mostly conducted to achieve the following:

- **Estimate Notify's readiness for release:**
  
  Measuring Notify's performance response time, throughput in a staging (production) environment. With the aim of ensuring that performance objectives are perfected before deployment.

- **Estimate and mitigate the performance impact of code changes:**

  Estimating the performance characteristics of an application after a change to the values of performance characteristics during previous runs (or baseline values), can provide an indication of performance issues (performance regression) or enhancements introduced due to a change[[1]](https://docs.microsoft.com/en-us/archive/blogs/dajung/ebook-pnp-performance-testing-guidance-for-web-applications).

- **Monitor and improve Notify's performance:**

  - Identifying hindrances and impediments with Notify against varying workload levels.

  - Measuring performance temperament of Notify against varying configurations.

  - Discover, test and conclude on scaling strategies for Notify.

- **Expedite capacity planning:**

  - Assist with the deciding on the type of hardware and software resources that are required to run Notify to support pre-defined performance goals.

  - Assist measuring business expectations against the periodic fluctuations of usage, especially as it concerns cost of running the hardware and software infrastructure.

**Requirements:**

* Test the performance of Notify along the following lines:
  * Notify Endpoint (Response time SLA, Acceptable error rates and Max connection).
  * Throughput (Change Event to Change Request delay).
  * Database (Connection on the database, are we overloading the database?).

## Considerations

### Locust Performance Testing Tool

Locust is a scriptable and scalable performance testing tool[[2]](http://docs.locust.io/en/stable/what-is-locust.html).

It requires definition of the performance expectation in regular Python code, and can be executed and presented via UI or headless via a commandline terminal, making it flexible and less restritive to domain specific language[[2]](http://docs.locust.io/en/stable/what-is-locust.html).

This makes Locust infinitely expandable and very developer friendly[[2]](http://docs.locust.io/en/stable/what-is-locust.html).

**Pros:**

* Python based tool which is the main programming language for this project.
* Notify Team know Python and is used for this project.
* Though single threaded, Locust benefits by using an async event loop (like NodeJS) to process the tests [[5]](https://github.com/cds-snc/notification-adr/pull/10/files#r761453356).
* It is light weight in its memory usage.
* Locust is hackable allowing the tracking of any Python command or external process.
* It is extenable via plugins.

**Cons:**

* Locust only test application via HTTP.
* As of the time of this ADR, Locust has fewer plugins when compare to JMeter.
* Locust's GUI is simple and can only be used for reporting.
* Have to use external projects to create improved graphs.
* Runs on a single thread, developer cannot blocking (long-running) operations cannot be processed, developer needs to beaware of this.

### Infrastructure 

#### Lambda Function

Initially we decided to use Lambda Function as the main entry point for running Notify performance test. 

AWS Lambda functions allow code to be run in a highly scalable way. They can integrate properly with AWS products and in this case we are required to run the performance tests once every day after which the results (HTML and CSV) are published to S3. This option was explored because: 

- It has less complexity for the Notify team to manage, the code was lesser compared to ECS.
- Costs lesser than ECS.

We initially went with this option, but soon found out that the file descriptors were capped and the performance tests were quickly running out of connections.


> POST,/v2/notifications/email,
"HTTPSConnectionPool(host='api.staging.notification.cdssandbox.xyz', port=443): 
Max retries exceeded with url: /v2/notifications/email 
(Caused by SSLError(OSError(24, 'No file descriptors available')))",1


#### Elastic Container Service(ECS)

Given the aforementioned experience with Lambda, we turned to ECS mainly because of the limited file descriptors available.

Amazon Elastic Container Service (Amazon ECS) offers high scalability, faster container management service, stop, and manage containers. Our Performance test containers are defined in a task definition that used to execute the performance test a service. The service configuration enables the performance test to run and maintain a pre-defined number of tasks. The tasks and services are on a serverless infrastructure that is managed by AWS Fargate [[3]](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html). 

Amazon ECS enables you to launch and stop our performance test container by using schedule expression defined in the task [[3]](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html).

The scheduled placement of performance test container are based on resource needs, isolation policies, and availability requirements [[3]](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html). 

Following the successful execution of the performance test, HTML and CSV results are published to a staging S3 bucket. 

An hour after the ECS has finished the performance test, the results are exported to Github via this workflow script [[4]](https://github.com/cds-snc/notification-performance-test-results/blob/main/.github/workflows/sync-performance-test-results.yml).


![Overview of the performance test container ECS agent](diagrams/2021-11-30.performance-testing/overview-notify-performance-test-containeragent.png?raw=true "Overview of the performance test container ECS agent")

## Decision

The decision was to go with:

-  Locust as it is the most compatible with the project and team. Being that the tool is Python and open source, it can be exploited to be the most suitable to performance test tool for testing throughput of the Notify application.
- Though Lambda is widely used and easier, we decided to go with the container agent solution from AWS ECS due to the challenge presented by the file descriptor issue.


## Consequences
TBD

## References

[1] Performance Testing Guidance for Web
Applications https://docs.microsoft.com/en-us/archive/blogs/dajung/ebook-pnp-performance-testing-guidance-for-web-applications

[2] http://docs.locust.io/en/stable/what-is-locust.html

[3] https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html

[4] https://github.com/cds-snc/notification-performance-test-results/blob/main/.github/workflows/sync-performance-test-results.yml

[5] https://github.com/cds-snc/notification-adr/pull/10/files#r761453356

