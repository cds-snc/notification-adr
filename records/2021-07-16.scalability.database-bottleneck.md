# Database performance issues

Date: 2021-07-16

## Status

**DRAFT**

## Context

Because Notify suffers from regular daily incident that originates from its
database when clients blasts notification, we need to review some design aspect
of the stack and bring more control to specific elements.

In the past months, the current architecture, environment and configuration have
been tweaked to achieve a certain balance that could take on the expected load.
To list a few of these tweaks:

* the number of maximum pool connection in individual application instance,
* the AWS instance classes,
* the resources requests and limits of Kubernetes containers,
* the horizontal pod autoscaling.

These serve well the application but there were already signs of the
application's limit: we rarely if ever meet our SLAs and we had regular daily
incidents that foretold of the difficulties ahead.

This document propose various options to address thorny issues that our
database is having.

## Timeline of an incident

1. A blast of notifications is sent to Notify through the API.
2. As the notification-api component saves the notifications to be sent in
the database, it commits each of these individually.
3. The notification is then sent to the appropriate queue, depending on its
type, scheduling and used key.
4. In the case of an email that was requested through the API, the email
will be sent to the AWS SES provider (the only supported at the time of
this writing).
5. As multiples emails are sent and some receive a response receipt, multiple
updates on the database are performed.
6. In the meantime, numerous count operations are performed to track the
current delivered emails and various metrics.
7. As there are more and more commits performed on the database, for each and
every notification, the database triggers an autovacuum action to clean up the
previous inserted rows by design of its
[Multiversion Concurrency Control](https://www.postgresql.org/docs/11/mvcc-intro.html).
8. The database performance decreases as autovacuum is running and slows down
the concurrent insertion, update and reading of the `notifications` table
records.
9. As the *notification-admin* and *notification-api* components are allegedly
slowing down due to a slower database, the healthcheck response[1] time gets higher
than normal and the ALB stop serving these instances, effectively making these
to stop receiving any request. It affects all instances of the API and Admin
which both heavily relies on calls to the same database.
10. As the database slowed down to answer the API, the latter does not stop
receiving and processing notifications. It continues to makes calls to the database
and consumes more database connections as previous ones are still busy.
11. Errors of maximum number of database connections getting reached start
to appear at that point.
12. The API instances are down as these are detached from the load balancers,
the database is overloaded with connections and can't take any more. **We are 
losing notifications** because we don't save these when the API is unavailable.

[1] The ALB makes health checks on Notify's stack components to make sure these
are eligible for traffic, but they call a `_status?simple=true` endpoint that makes
sure a database call is not included as part of the check. This means the situation
might be more dire as the instance overall slowed down so much due to the database
might lock the process due to its unavailability.

_TODO: Add sequence or flow chart_

## Options

### Stream the notifications save

Upon receiving a notification from the API, do not save it in the current
*notification-api* process but defer it to a celery task.

### Insert the notifications as a batch

As we streamline the notification save with the previous option and got
these under Celery land, we can now batch these easily. There is a 
[celery-batch](https://pypi.org/project/celery-batches/) project which 
exists that might offer us the option to easily achieve this with minimal
setup.

### Tweaking autovacuum options

There are a few autovacuum configuration that we can tweak in order to delay
the autovacuum action that triggers a performance slowdown of our database:

* **autovacuum_scale_factor** (default: 0.2): we could use this to delay the vacuum, (suggestion 0.3).
* **cost_delay**: We could use this to delay by increasing the threshold.

### Manual execution of the vacuum process on the `notifications` table

An option is to keep the autovacuum enabled but trigger a manual vacuum prior
to traffic time. This would better prepare the database for upcoming load.

Don't run `vacuum FULL`: this will take on most database resources and prevent
proper operation of the database.

### Disable the autovacuum process on the `notifications` table

To avoid the autovacuum to process the `notifications` table during the daily
blast of notifications, we could disable the autovacuum to prevent it from
triggering. This need to be done carefully, as a maximum limit of 4 billion
transactions can occur in PostgreSQL. The autovacuum cleanup reset this
transaction number. If this threshold is reached, our database will be severely
affected.

This is a risky move and certainly not a long/mid term solution.

### Notifications table sharding

## Decision

_To be completed._

## Consequences

_To be discussed._
