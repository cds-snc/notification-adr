# Database performance issues

Date: 2021-07-16

## Status

**ACCEPTED**

## Context

Because Notify suffers from regular daily incident that originates from its
database when clients blasts notification, we need to review some design aspect
of the stack and bring more control to specific elements.

In the past months, the current architecture, environment and configuration have
been tweaked to achieve a certain balance that could take on the expected load.
To list a few of these tweaks:

* the number of maximum pool connection in individual application instance,
* the AWS instance classes,
* the resources requests and limits of Kubernetes containers,
* the horizontal pod autoscaling.

These serve well the application but there were already signs of the
application's limit: we rarely if ever meet our SLAs and we had regular daily
incidents that foretold of the difficulties ahead.

This document propose various options to address thorny issues that our
database is having. [Click here for schema of our database](diagrams/2021-07-16.scalability.database-bottleneck/relationships.real.compact.png)
at the time of this writing.

## Timeline of an incident

1. A blast of notifications is sent to Notify through the API.
2. As the notification-api component saves the notifications to be sent in
   the database, it commits each of these individually[1].
3. The notification is sent to the appropriate queue, depending on its
   type, scheduling and used key.
4. In the case of an email that was requested through the API, the email
   will be sent to the AWS SES provider (the only supported at the time of
   this writing). An UPDATE operation is performed for the corresponding
   notification in the database to change the status to `sent` and updating the
   timestamp for the `sent_at` column.
5. As multiples emails are sent, each are getting a response receipt.
   An UPDATE operation is performed for the corresponding notification in the
   database to change the status to `delivered` and updating the timestamp for the
   `updated_at` column.
6. In the meantime, numerous count operations are performed to track the
   current delivered emails and various metrics.
7. As there are more and more commits performed on the database, for each and
   every notification, the database triggers an autovacuum action to clean up the
   previous inserted rows by design of its
   [Multiversion Concurrency Control](https://www.postgresql.org/docs/11/mvcc-intro.html).
8. The database performance decreases as autovacuum is running and slows down
   the concurrent insertion, update and reading of the `notifications` table
   records.
9. As the *notification-admin* and *notification-api* components are allegedly
   slowing down due to a slower database, the healthcheck response[2] time gets higher
   than normal and the ALB stop serving these instances, effectively making these
   to stop receiving any request. It affects all instances of the API and Admin
   which both heavily relies on calls to the same database.
10. As the database slowed down to answer the API, the latter does not stop
   receiving and processing notifications. It continues to makes calls to the database
   and consumes more database connections as previous ones are still busy.
11. Errors of maximum number of database connections getting reached start
   to appear at that point.
12. The API instances are down as these are detached from the load balancers,
   the database is overloaded with connections and can't take any more. **We are
   losing notifications** because we don't save these when the API is unavailable.

[1] An individual commit in a database in not a problem usually, but when
several of these occurs, potentially over 100,000 in the case of Notify in time
of high traffic, it becomes a performance issue. An apt analogy is possible
with someone getting to the cashier in a grocery store to pay their items:
imagine that you would have to pay for each these items individually, i.e.
a) scan one and only one item, b) ask for the total, c) pay with cash or card,
d) repeat for all remaining items. This will take quite a long time compared
to paying for all items in one batch within one transaction. Saving one
notification every time in the database is akin to paying for one grocery item
individually in one transaction, except you have potentially thousands of items
remaining afterward. Ideally, we want to buy/save all for all of these in one
go.

[2] The EKS environment listens [on the `readinessProbe` health check](https://github.com/cds-snc/notification-manifests/blob/6ac406b315112d2b66d93d8fa74406c3e6e7639f/base/api-deployment.yaml#L216)
that our setup exposes and it involves a direct database check. When the
database slows down, so does the probe. The direct reaction from EKS is to
detach the instances from the application's load balancers.

![Incident Timeline Sequence Flow](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-07-16.scalability.database-bottleneck/timeline-incident.png?raw=true "Incident Timeline Sequence Flow")

## Options

### Stream the notifications save

Upon receiving a notification from the API, do not save it in the current
*notification-api* process but defer it to a celery task. This does not
address the root cause of our issue but offer many benefits, namely:

1. This is an iterative improvement where we send the database save into
   [Celery land](https://docs.celeryproject.org/en/stable/). Having these into
   a Celery tasks means we can more easily control the flow of these operations
   and control these with all of the Celery features. Specifically, we could batch
   the notifications using [celery-batch](https://pypi.org/project/celery-batches/)
   as an example, but any mean to achieve the saving of notification in batch
   would be sufficient.
2. The number of database connections will be lowered in the API component
   as the saves moved into a Celery task. This will allow room for a faster
   processing API. The API should not process heavy or slow processing
   operations as this can prevent proper serving of HTTP requests. Ideally,
   it should apply minimal transformation and store into a queue as quick as
   possible (or other distributed data structures) that will offer resilience,
   fault tolerance and scalability.

### Insert the notifications as a batch

As we streamline the notification save with the previous option and got
these under Celery land, we can now batch these easily. There is a
[celery-batch](https://pypi.org/project/celery-batches/) project which
exists that might offer us the option to easily achieve this with minimal
setup.

### Tweaking autovacuum options

There are a few autovacuum configuration that we can tweak in order to delay
the autovacuum action that triggers a performance slowdown of our database:

* **autovacuum_scale_factor** (default: 0.2): we could use this to delay the vacuum, (suggestion 0.3).
* **cost_delay**: We could use this to delay by increasing the threshold.

### Manual execution of the vacuum process on the `notifications` table

An option is to keep the autovacuum enabled but trigger a manual vacuum prior
to traffic time. This would better prepare the database for upcoming load. Much
of the traffic occurs around 13h-15h EST but we can't expect that will always
be the case. This can't be a long term solution or even a solution by itself
but rather soothing on short term to explore how autovacuum affects the Notify
stack overall.

Testing this would be interesting to know if we autovacuum would kick in with
preventive manual runs. This could provide some insights.

Don't run `vacuum FULL`: this will take on most database resources and prevent
proper operation of the database.

### Disable the autovacuum process on the `notifications` table

This could be helpful to diagnose the problem: we could see how the database
and the Notify stack behave without autovacuum getting in the way. This is a
risky move and certainly not a solution by itself.

This need to be done carefully, as a maximum limit of 4 billion transactions
can occur in PostgreSQL. The autovacuum cleanup reset this transaction number.
If this threshold is reached, our database will be severely affected.

### Database connections pooling

To maximise the distribution of database connection, we can have a
pooler that would sit in-between the Notify stack and the AWS RDS
Aurora Postgres instances.

RDS Proxies would exists for this purpose. Apparently, we could create an
endpoint each for both readers and the writer. This could co-exist nicely
with our existing application logic which dispatch database sessions to
either the readers or writer.

### Tweak the readiness and liveness Kubernetes probe

About these two probes: liveness probe will check if your pod is
up abnd running; failure to do so will restart it. Readiness will check if
your pod can receive traffic; failure to do so will detach the pod from the
load balancer. Both probes will be continually checked throughout the
pods' life.

At the moment, the liveness probe targets the simple healthcheck that skips
the database check while the readiness targets the healthcheck with the
database check.

Because our readiness probe reports that our pods are not ready anymore and
hence, get detached from the load balancer, which **then cause notifications to
be lost forever**, we could tweak it on the short term to accommodate our current
situation and stop the bleeding.

Normally, a delayed database should not be normal. We know in our current
situation though that the database is still up and running. We'll receive paging
alerts anyway if it goes down. We should definitely put back the database
check once the design is fixed.

### Notifications table sharding

A [database shard](https://en.wikipedia.org/wiki/Shard_(database_architecture)), or simply a shard, is a horizontal partition of data in a database or search engine. Each shard is held on a separate database server instance, to spread load.

Some data within a database remains present in all shards,[a] but some appears only in a single shard. Each shard (or server) acts as the single source for this subset of data.[1]

### [When should we resort to sharding?](https://www.percona.com/blog/2019/05/24/an-overview-of-sharding-in-postgresql-and-how-it-relates-to-mongodbs/)

Here are a couple of classic cases:

* To scale out (horizontally), when even after partitioning a table the amount of data is too great or too complex to be processed by a single server.
* Use cases where the data in a big table can be divided into two or more segments that would benefit the majority of the search patterns. A common example used to describe a scenario like this is that of a company whose customers are evenly spread across the United States and searches to a target table involves the customer ZIP code. A shard then could be used to host entries of customers located on the East coast and another for customers on the West coast.

Note though this is by no means an extensive list.

## Decision

Our current recommendations at the time of this writing:

1. Stream the notifications save from the API to a Celery task.
2. Insert the notifications as a batch once #1 is done.
3. Test and implement, if possible, the RDS proxies for connection pooling.
4. Explore the disabling/tweaking of the autovacuum option in the staging
   environment under heavy load tests.

## Consequences

_To be discussed._
