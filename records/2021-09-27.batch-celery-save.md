# Database performance issues

Date: 2021-09-27

## Status

**PROPOSED.**

## Context

Following the ADR around scalability this previous July, one of the recommendation
is to perform a batch save of the notifications when these are incoming into our
API component rather than saving one by one. [A broader picture is portrayed in the
ADR itself](https://github.com/cds-snc/notification-adr/blob/main/records/2021-07-16.scalability.database-bottleneck.md)
with a larger context.

An individual commit in a database in not a problem usually, but when
several of these occurs, potentially over 100,000 in the case of Notify in time
of high traffic, it becomes a performance issue.

For a recap, imagine you are at the grocery store. You get to teh cashier to pay
your items. You have to pay for each these items individually, i.e.
a) scan one and only one item, b) ask for the total, c) pay with cash or card,
d) repeat for all remaining items. This will take quite a long time compared
to paying for all items in one batch within one transaction. Saving one
notification every time in the database is akin to paying for one grocery item
individually in one transaction, except you have potentially thousands of items
remaining afterward. Ideally, we want to buy/save all for all of these in one
go.

### Current notification flow through Celery tasks

> **INFO**: [Celery](https://docs.celeryproject.org/en/stable/getting-started/introduction.html) is a Python library to process a message through a workflow of tasks which offers reliability mechanism (e.g. retry), monitoring, concurrency (to execute multiple work at the same time), automatic scheduling of tasks and allows for ways to scale up the processing.

Our current workflow of Celery tasks for processing an email or SMS at the moment
is simplified with the next diagram.

![Current workflow of a notification save](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/current-workflow.jpg?raw=true "Current workflow of a notification save")

## Options

Listing of potential solutions and necessary architectural changes will be listed in
this section. Feel free to list your own idea in here via a proposed change to this
document.

### [celery-batches](https://github.com/clokep/celery-batches)

Around the development of [Celery version 3](https://docs.celeryproject.org/en/3.1/reference/celery.contrib.batches.html), there was an option to batch the content
of a queue and have a task flush it all when one of the two constraint would be met:
the number of maximum requests or a timeout. For example, with a limit of 1000
requests maximum and a timeout of 5 seconds, if 1,000 requets would be filled in 2
seconds, then the task would trigger and flush all of the 1,000 requests buffered
in the current task. Alternatively, if only 2,300 requests filled up and the 5
seconds timeout completed, then all of the 2,3000 requests would get flushed to
the task.

This is a transparent and easy mechanism that could get leveraged in Celery in order
to achieve the batch save. Unfortunately, that feature was always in an experimental
state in Celery version 3 and disappeared in subsequent versions. An individual
contributer on GitHub [made a fork of the experimental feature](https://github.com/clokep/celery-batches) and adapted for recent
versions of Celery. This looked promising unfortunately there are two issues by
using this library:

1- It still can be considered into an experimental stage as this does not supports common basic feature offered by Celery such [as a retry mechanism](https://github.com/clokep/celery-batches/issues/10).
2- The integration is not that easy with Flask (although probably not impossible) as described by Flask integration recipe [in their documentation](https://flask.palletsprojects.com/en/2.0.x/appcontext/). [An issue was opened](https://github.com/clokep/celery-batches/issues/35) for curiosity sake to see if that could be done but even then, the issue described at #1 is a blocker.

### Send notifications in queue and sweep process from a heartbeat process

> **INFO**: There is a [Hearbeat](https://docs.celeryproject.org/en/stable/internals/reference/celery.worker.heartbeat.html) service within Celery that the latter manages. This is a one and only execution thread which allows to orchestrate certain tasks based on a schedule. This is useful as we can leverage a memory model that we will not share. Hence we avoid the pitfalls of concurrent execution thread trying to access shared memory all at the same time and which requires complex locking mechanism. This is similar to the UI model with an unique UI thread and its exclusive access to UI data, or the actor model which has one actor keeping its one exclusive state kept away from other actors.

In order to achieve batch save, we need to buffer and retrieve the items from the
Celery task. There is unfortunately no known mechanism at this time of writing of
Celery that allows the retrieval of all items for a queue and process it as a batch
(other than the previously mentioned and defunct celery-batch experimental library).

There are [Celery primitives that allows to chain, iterate concurrently and group](https://docs.celeryproject.org/en/stable/userguide/canvas.html#the-primitives)
which will be necessary to achieve a custom solution but no aggregate operation is
available.

Hence, we will need to buffer our notifications into a queue or cache system when
we receive the notifications, prior to save these.

![Buffer workflow of a notification save](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/buffer-workflow.jpg?raw=true "Buffer workflow of a notification save")

We can do so by either storing the notifications in a queue (such as AWS SQS) or a
distributed cache (such as AWS Elasticache Redis). Once these are stored, the
heartbeat celery execution thread could check the stored notifications every
second with a maximal number of notifications with an arbitrary number at first
(i.e. 500 notifications) and properly weighted after performance tests runs.

The `bulk_save_email` Celery task is new as this would bulk insert the notifications
into the database but it would leverage the existing code and tasks. It would chain
the newly created bulk notifications into the Celery tasks like the current workflow
does; nothing new on that side.

We should then consider which data store to use as a buffer for the Celery beat
execution thread to feed upon: Redis or SQS? Let's break these down into comparison
tables.

#### AWS Elasticache Redis vs AWS SQS

| Features               | RDS | SQS |
|------------------------|-----|-----|
| Response Time          |     |     |
| Bandwitch capacity     |     |     |
| Concurrent consumption |     |     |
| Metric: GET/s          |     |     |
| Metric: average age    |     |     |

## Decision

_To be determined._

## Consequences

_To be discussed._
