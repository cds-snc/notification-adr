# Batch save of notifications

Date: 2021-09-27

## Status

**Approved.**

## Context

Following the ADR around scalability this previous July, one of the recommendation
is to perform a batch save of the notifications when these are incoming into our
API component rather than saving one by one. [A broader picture is portrayed in the
ADR itself](https://github.com/cds-snc/notification-adr/blob/main/records/2021-07-16.scalability.database-bottleneck.md)
with a larger context.

An individual commit in a database in not a problem usually, but when
several of these occurs, potentially over 100,000 in the case of Notify in time
of high traffic, it becomes a performance issue.

For a recap, imagine you are at the grocery store. You get to the cashier to pay
your items. You have to pay for your items individually, i.e.
a) scan one and only one item, b) ask for the total, c) pay with cash or card,
d) repeat for all remaining items. This will take quite a long time compared
to paying for all items in one batch within one transaction. Saving one
notification every time in the database is akin to paying for one grocery item
individually in one transaction, except you have potentially thousands of items
remaining afterward. Ideally, we want to buy/save all for all of these in one
go.

### Current notification flow through Celery tasks

> **INFO**: [Celery](https://docs.celeryproject.org/en/stable/getting-started/introduction.html) is a Python library to process a message through a workflow of tasks which offers reliability mechanism (e.g. retry), monitoring, concurrency (to execute multiple work at the same time), automatic scheduling of tasks and allows for ways to scale up the processing.

Our current workflow of Celery tasks for processing an email or SMS at the moment
is simplified with the next diagram.

![Current workflow of a notification save](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/current-workflow.jpg?raw=true "Current workflow of a notification save")

## Options

Listing of potential solutions and necessary architectural changes will be listed in
this section. Feel free to list your own idea in here via a proposed change to this
document.

### [celery-batches](https://github.com/clokep/celery-batches)

Around the development of [Celery version 3](https://docs.celeryproject.org/en/3.1/reference/celery.contrib.batches.html), there was an option to batch the content
of a queue and have a task flush it all when one of the two constraint are met:
the number of maximum requests or a timeout. For example, with a limit of 1000
requests maximum and a timeout of 5 seconds, if 1,000 requets would be filled in 2
seconds, then the task would trigger and flush all of the 1,000 requests buffered
in the current task. Alternatively, if only 230 requests filled up and the 5
seconds timeout completed, then all of the 230 requests would get flushed to
the task.

This is a transparent and easy mechanism that could get leveraged in Celery in order
to achieve the batch save. Unfortunately, that feature was always in an experimental
state in Celery version 3 and disappeared in subsequent versions. An individual
contributer on GitHub [made a fork of the experimental feature](https://github.com/clokep/celery-batches) and has adapted the latter for recent
versions of Celery. This looked promising unfortunately there are two issues by
using this library:

1. It still can be considered in an experimental stage as this does not support common basic features offered by Celery such [as a retry mechanism](https://github.com/clokep/celery-batches/issues/10).
2. The integration is not that easy with Flask (although probably not impossible) as described by the Flask integration recipe [in their documentation](https://flask.palletsprojects.com/en/2.0.x/appcontext/). [An issue was opened](https://github.com/clokep/celery-batches/issues/35) for curiosity sake to see if that could be done but even then, the issue described in #1 is a blocker.

### Send notifications in queue and sweep process from a beat process

> **INFO**: There is a [periodic task](https://docs.celeryproject.org/en/stable/userguide/periodic-tasks.html#:~:text=celery%20beat%20is%20a%20scheduler,entries%20in%20a%20SQL%20database) scheduler within Celery that the latter manages. This is a one and only execution thread which allows to orchestrate certain tasks based on a schedule. This is useful as we can leverage a memory model that we will not share. Hence we avoid the pitfalls of concurrent execution thread trying to access shared memory all at the same time and which requires complex locking mechanism. This is similar to the UI model with an unique UI thread and its exclusive access to UI data, or the actor model which has one actor keeping its one exclusive state kept away from other actors.

In order to achieve batch save, we need to buffer and retrieve the items from the
Celery task. There is unfortunately no known mechanism at this time of writing in
Celery that allows the retrieval of all items for a queue and process them as a batch
(other than the previously mentioned and defunct celery-batch experimental library).

There are [Celery primitives that allows to chain, iterate concurrently and group](https://docs.celeryproject.org/en/stable/userguide/canvas.html#the-primitives)
which will be necessary to achieve a custom solution but no aggregate operation is
available.

Hence, we will need to buffer our notifications into a queue or cache system when
we receive the notifications, prior to saving.

![Buffer workflow of a notification save](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/buffer-workflow.jpg?raw=true "Buffer workflow of a notification save")

We can do so by either storing the notifications in a queue (such as AWS SQS) or a
distributed cache (such as AWS Elasticache Redis). Once these are stored, the
celery execution thread could check the stored notifications every
second with a maximal number of notifications with an arbitrary number at first
(i.e. 500 notifications) and properly weighted after performance tests runs.

The `bulk_save_email` Celery task is new as this would bulk insert the notifications
into the database but it would leverage the existing code and tasks. It would chain
the newly created bulk notifications into the Celery tasks like the current workflow
does; nothing new on that side.

We should then consider which data store to use as a buffer for the Celery beat
execution thread to feed upon: Redis or SQS? Let's break these down into the next section.

### Redis List vs Redis Stream vs AWS SQS FIFO vs AWS SQS Regular

There many advantages to each technology and it isn't clear which one is the
best fit for our needs. Let's break down many desired features into a
comparison table. There are a few concerns and pitfalls to enumerate that we
need to take in account.

1. **Latency**: The general latency as reported by people who used either technology.
1. **Throughput**: The number of batch throughput and overall physical bandwith.
1. **Concurrency**: If the beat task wakes up every 5 seconds and the previous execution did not finish yet, the messages from the previous run should not be consumed again.
1. **Persistence**: The messages are persisted even with the queue system going down or the workers being down and letting messages to accumulate.
1. **Loss protection**: Guarantees that non-processed messages due to worker's failure will get processed by another worker later.
1. **Exactly once processing**: The notifications are sent to the worker once and exactly once, to guarantee all messages to be processed and avoid duplicates.
1. **CloudWatch Staleness Alarm**: Offers a mean to report a non-processed queue via a CloudWatch alarm.
1. **Metrics: _Name_**: Track mentioned named metrics.

|                                | Redis List                                                                               | Redis Stream                                                               | SQS FIFO                                                                                                                                                                                                                                                                                                                       | SQS Standard                                           |
|--------------------------------|------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|
| **Latency**                    | 250 messages is ~0.001 seconds                                                           | ?                                                                          | 250 messages is ~0.6 seconds                                                                                                                                                                                                                                                                                                   | ? (faster than 250 messages is ~0.6 messages)          |
| **Throughput**                 | No limits on number of messages, no message size limit                                   | No limits on number of messages, no message size limit                     | 10 messages / request, 256k max, 3000/s with [high throughput enabled](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/high-throughput-fifo.html), [**not available in ca-central-1**](https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-sqs-supports-high-throughput-fifo-queues-preview/) | 10 messages / request, 256k max, no limit              |
| **Concurrency**                | Single-threaded execution + atomic operations                                            | Via block and last ID or via consumer groups                               | Via visibility feature on consumption + timeout                                                                                                                                                                                                                                                                                | Via visibility feature on consumption + timeout        |
| **Persistence**                | Via logs (needs extra code) & dump is possible                                           | Via logs (need extra code) & dump is possible                              | 14 days max for stored messages. In-flight: 20,000 for FIFO and                                                                                                                                                                                                                                                                | In-flight has 120,000 maximum capacity                 |
| **Dead letter**                | Deadletter can be emulated via additional list                                           | ?                                                                          | Deadletter support                                                                                                                                                                                                                                                                                                             | Deadletter support                                     |
| **Loss protection**            | Emulated with a in-flight+timeout feature via additional list and LMOVE atomic operation | Can be emulated via counter in XPENDING to move into other consumer groups | In-flight+timeout allows for lost messages to reappear                                                                                                                                                                                                                                                                         | In-flight+timeout allows for lost messages to reappear |
| **Exactly once processing**    | Yes                                                                                      | Yes                                                                        | Yes                                                                                                                                                                                                                                                                                                                            | No, i.e. at least once processing                      |
| **Metric: GET/s**              | Available via AWS dashboard                                                              | Available via AWS dashboard                                                | Available via AWS dashboard                                                                                                                                                                                                                                                                                                    | Available via AWS dashboard                            |
| **Metric: average age**        | No                                                                                       | No                                                                         | Available via AWS dashboard                                                                                                                                                                                                                                                                                                    | Available via AWS dashboard                            |
| **CloudWatch Staleness Alarm** | Via items number threshold and raising alarm when reached                                | Via items number threshold in stream and raising alarm when reached        | Via average time threshold                                                                                                                                                                                                                                                                                                     | Via average time threshold                             |

A few observations from this comparison table:

1. SQS offers the best in all option for error handling, resilience and persistence. The big downside of the SQS option is the limit of maximum message retrieval set at 10 messages per request. This means that many network requests will need to be made and extra code to write / maintain / monitor to spin up many workers in order to achieve the desired throughput (i.e. at least 500 requests / second bare minimum and this should scale in the future).
2. Redis does not have such strong error handling, resilience and persistence options such as SQS, but it offers strong building blocks to emulate these. It is also blazingly fast and does not have a limit on the number of messages we can retrieve per request.
3. SQS has much better guarantee for queuing up massive amount of messages as it is backed up by a filesystem whereas Redis, by default, will be limited by the memory it has access to. **Normally**, the queue should not accumulate many messages, but in a scenario where the workers or load balancers are down, it will accumulate and potentially get over its memory capacity, which will lead to a **loss of message**. Hence, strong safeguards and alerts need to be around these limits to make sure proper actions are taken as quick as one can.
4. AWS has a limited offering for the Canadian market for high throughput FIFO SQS queues unfortunately.
5. AWS does not offer a ready solution to monitor an individual Redis data structure to raise proper alarms when too many items are growing in it. It does offer alerts around memory usage though. One could write a lambda that checks the item counts of the Redis data structure at a time interval to publish a custom metric though.

#### Resources

* [Redis Stream vs. Amazon SQS](https://hackernoon.com/redis-stream-vs-amazon-sqs-g21n3y7p)
* [High throughput for FIFO queues](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/high-throughput-fifo.html)
* [Amazon SQS Now Supports a High Throughput Mode for FIFO Queues](https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-sqs-supports-high-throughput-fifo-queues-preview/)
* [Amazon SQS visibility timeout](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html)

### SQS Sequence Workflow

This section covers the necessary sequence operations for SQS (FIFO or standard) for both normal and failure
scenarios. This acknowledge the usage of in-flight mode, natively supported in the case of SQS.

In the case of a normal scenario, the `SQS inbox` mentioned in the sequence diagrams represents the same
storage as `Notifs Buffer Queue / Cache` in the previous section workflows.

![Sequence diagram of a normal batch save operation using SQS](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/seq-sqs-flow-normal.png?raw=true "Sequence diagram of a normal batch save operation using SQS")

A key element of this sequence diagram is the operation that marks the messages as in-flight, which will render
the messages invisible to any other consumers. Once the messages are retrieved, it is the responsibility of the
consumer to delete the messages manually once it is done processing. This indicates to the queue that the messages
are done with. Failure to delete the messages will be considered as a failure from the consuming process and
hence, will render the messages visible again to potential consumer after a pre-configured timeout period.

The scenario of a failure illustrates how the in-flight state will play in our favor if something wrong happens.

![Sequence diagram of a failed batch save operation using SQS](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/seq-sqs-flow-failure.png?raw=true "Sequence diagram of a failed batch save operation using SQS")

Once the failure happened, the deletion of the messages is skipped. The messages will be picked up normally
after the timeout period and the whole batch save process will restart with the previous batch.

### Redis Sequence Workflow

This section covers the necessary sequence operations for Redis using List as a data structure to emulate a queue
mechanism for both normal and failure scenarios. This acknowledge the usage of in-flight mode, supported with an
emulated in-flight list in the case of Redis.

In the case of a normal scenario, the `Redis inbox` mentioned in the sequence diagrams represents the same
storage as `Notifs Buffer Queue / Cache` in the previous section workflows.

![Sequence diagram of a normal batch save operation using Redis](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/seq-redis-flow-normal.png?raw=true "Sequence diagram of a normal batch save operation using Redis")

Observe the second in-flight Redis list: this one replicates the same functionality that AWS SQS offers. It will
need to be emulated with some extra code but Redis offers a strong atomic instruction to guarantee safe concurrent
execution via [the LMOVE command](https://redis.io/commands/lmove). Hence, we can move the messages to consume
from the inbox to the in-flight queue and get these return in one sweep. No other concurrent consumers should
hence get the same message.

If the batch save fails, then the messages will stay in the in-flight list. There would be a beat celery
task to check on this list at an time interval (e.g. 5 minutes) that moves messages older than that time interval
back into the initial inbox list, ready to be processed again by the next batch save iteration.

![Sequence diagram of a failed batch save operation using Redis](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/seq-redis-flow-failure.png?raw=true "Sequence diagram of a failed batch save operation using Redis")

This demonstrates how Redis can achieve the same level of error handling as AWS and a somewhat similar resilience,
aknowledging that it does not offer a filesystem based persistence in case of a Redis crash or out of memory
error.

## Decision

We are going forward with the following items:

* Enqueue items into a batch queue for items to be batch saved;
* Use an in-flight mechanism to protect against potential loss of notifications;
* Use Redis as a queue mechanism given it's high throughput rate, fast response time and overall faster solution;
* Protect the batch save into a feature flag for its initial implementation;
* Monitor the number of notifications in the batch save queue and in-flight mode;
* Have warning and critical alarms setup within CloudWatch that will triggers our paging support notifications.

## Consequences & Results

* running on our staging Kubernetes cluster with batch size 10
* locust users POST to `/email` every 30 seconds
* running for 10 minutes
* using a locust command similar to

```sh
locust -f ./locust-notifications.py --headless --users=4000 --spawn-rate=100 --run-time=10m --html=k8s_8000_ff_on.html
```

### 2000 emails / minute, batch saving off

![2000 emails / minute, batch saving off](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/k8s_2000_ff_off.png?raw=true)

### 2000 emails / minute, batch saving on

![2000 emails / minute, batch saving off](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/k8s_2000_ff_on.png?raw=true)

### 4000 emails / minute, batch saving off

![4000 emails / minute, batch saving off](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/k8s_4000_ff_off.png?raw=true)

### 4000 emails / minute, batch saving on

![4000 emails / minute, batch saving off](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/k8s_4000_ff_on.png?raw=true)

### 8000 emails / minute, batch saving off

![8000 emails / minute, batch saving off](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/k8s_8000_ff_off.png?raw=true)

### 8000 emails / minute, batch saving on

![8000 emails / minute, batch saving off](https://raw.githubusercontent.com/cds-snc/notification-adr/main/records/diagrams/2021-09-27.batch-celery-save/k8s_8000_ff_on.png?raw=true)
